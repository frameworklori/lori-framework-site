# LII: Linguistic Incendiary Index

 ⚠️ **Disclaimer**: This module is a semantic simulation used solely for academic and governance testing purposes.
> All figures, phrases, and examples are anonymized and hypothetical. No reference to real individuals or ideologies is intended.

> Measures how likely a piece of language (spoken, written, AI-generated) may trigger emotional conflict, imitation, or social unrest.

---

<p align="center">
<!-- B. GitHub 預覽用絕對路徑 -->
<img src="https://github.com/frameworklori/lori-framework-site/blob/main/docs/assets/images/linguistic-incendiary-index.png?raw=true" alt="LII diagram" width="400">
</p>

> **This 2D map shows how emotionally charged language (Trigger) interacts with social amplification forces (Amplification). When both are high, language spreads rapidly and mimics behavior.**


## Core Concepts

- **Incendiary Term Detection**
Identifies emotionally provocative words or frames.

- **Cultural Sensitivity Context**
Analyzes if the phrase triggers different meanings across groups.

- **Amplification Risk Score**
Predicts if the language is likely to go viral, mislead, or incite actions.

---

## Use Cases

- Detecting hate speech veiled in abstract language
- Evaluating AI-generated content risks
- Platform moderation and narrative conflict forecasting

---

## Linked Modules

- [EDRI-H](EDRI-H.md)
- [Trust Drift Map](TrustDrift.md)


[🔙 GO BACK to Main Framework Page](https://frameworklori.github.io/lori-framework-site)
